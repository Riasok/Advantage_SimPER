# Copyright (c) 2023 Contextual AI, Inc.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.
"""
Contains the functions for loading data.
Each function of the form get_{dataset_name} (e.g., get_shp, get_oasst, etc.) will return a dict of Example objects, indexed by the prompt for the text.

Each Example object will contain
- the prompt
- a list L of generations
- the index in L of the generation that should be the finetuning target
- a list S of the scores for the generations
- for preference feedback data: pairs of indices (i,j) in L, where generation i is preferable to generation j
- for binary feedback data: whether each generation is desirable/chosen or undesirable/rejected
- whether to truncate the beginning or end if the maximum number of tokens is exceeded
- the dataset name
- the unformatted prompt
"""

import datasets
import torch
from torch.nn.utils.rnn import pad_sequence
from collections import defaultdict
import tqdm
import re
import random
import json
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from .utils import rank0_print, on_rank0, delete_dict
import pandas as pd
import numpy as np
import math
@dataclass
class Example:
    """
    Class for an example in a preference or SFT dataset. If you want each prompt to be uniquely associated with an Example instance, save it in a dict.
    """
    prompt: List = field(default_factory=list)                  # list of turns, each with two keys: "role" and "content"
    generations: List = field(default_factory=list)             # list of list of turns (the output sequences to predict)
    sft_index: int = -1                                         # which response in self.generations should be generated for SFT
    scores: List[float] = field(default_factory=list)           # score for each generation
    pairs: List[Tuple[int, int]] = field(default_factory=list)  # for preference feedback data: indices in responses, where i > j in pair (i,j) is a preference
    desirable: List[bool] = field(default_factory=list)         # for binary feedback data: whether the generation at the corresponding index in self.generations is desirable 
    truncation_mode: str = 'keep_end'                           # if truncation needed, keep the beginning (keep_start) or end (keep_end) (only override default for SHP)
    dataset_name: str = ''
    original_prompt: str = ''                                   # the unformatted prompt (needed to recover instruction for AlpacaEval)
    answer: str = ''                                            # ground truth answer for math-problem solving
    advantage: float = 0.0                                      # Advantage value

    def num_generations(self):
        return len(self.generations)
    
    def remove_extra_spaces(self):
        """
        Remove double spaces in the prompt and generations to standardize spacing.
        """
        def clean(text: str) -> str:
            return re.sub(r'[ \t]{2,}', ' ', text)

        # Clean the prompt
        for turn in self.prompt:
            turn['content'] = clean(turn['content'])

        # Clean the generations
        for x in self.generations:
            for turn in x:
                turn["content"] = clean(turn["content"])


class Dataset:
    """
    A collection of Example instances, indexed by prompt.
    """
    def __init__(self, name):
        self.name = name
        self.data = defaultdict(Example)

    def __setitem__(self, key, value):
        if not isinstance(key, str):
            raise KeyError("key must be a string")
        
        if not isinstance(value, Example):
            raise ValueError("value must be a Example")
        
        self.data[key] = value

    def __getitem__(self, key):
        return self.data[key]

    def __len__(self):
        return len(self.data)
    
    def __iter__(self):
        return iter(self.data)



def get_sampled_data(samples_path: str, split: str) -> Dataset:
    """
    Load samples generated by train.sample and convert it into a Dataset.
    """
    rank0_print(f'Loading samples from {samples_path}...')

    # Load all sample data
    with open(samples_path, 'r') as f:
        sample_data = json.load(f)

    data = Dataset('samples')

    for sample in sample_data:
        if sample.get('split', split) != split:
            continue

        prompt_key = str(sample['prompt_id']) + ' ' + str(sample['sample_id'])
        data[prompt_key].prompt = sample['prompt']
        data[prompt_key].generations.append([{"role": "assistant", "content": sample['output']}])
        data[prompt_key].dataset_name = sample['dataset']
        data[prompt_key].sft_index = 0

    return data    


def get_feedback(feedback_path: str, split: str) -> Dataset:
    """
    Load feedback data created by label.py and convert it into a Dataset.
    Supports both binary and pairwise feedback formats.

    Args:
        feedback_path: path to the JSON file containing feedback data
        split: only include objects whose 'split' value matches the given split

    Returns:
        A Dataset instance containing the feedback data.
    """
    rank0_print(f'Loading feedback dataset from {feedback_path}...')
    
    # Load all feedback data
    with open(feedback_path, 'r') as f:
        feedback_data = json.load(f)
    
    if not feedback_data:
        raise ValueError(f"No feedback data found in {feedback_path}")
    
    data = Dataset('feedback')

    # Group samples by flattened prompt to ensure we handle all samples for each prompt together
    grouped_samples = defaultdict(list)
    for item in feedback_data:
        prompt_key = ' '.join([ x['content'] for x in item['prompt'] ]) # flattened prompt
        grouped_samples[prompt_key].append(item)
    
    for prompt_key, samples in grouped_samples.items():
        if not samples: continue

        feedback_type = samples[0].get('type', None)   
        if not feedback_type:
            raise ValueError("Feedback type not specified in data") 
        
        # split is same for all examples with the same prompt
        if samples[0].get('split', split) != split:
            continue
        
        if feedback_type == 'binary_feedback':
            example = Example()
            
            # Add each sample's output and its desirability based on the label
            for sample in samples:
                example.prompt = sample['prompt']
                example.generations.append(sample['output'])
                example.desirable.append(bool(sample['label']))
                example.scores.append(sample['reward'])
                example.dataset_name = 'feedback'
                if "advantage" in sample.keys():
                    example.advantage = sample['advantage']
            
            # For binary feedback, use any desirable response as the SFT target
            # If no desirable responses, use the highest scoring one
            desirable_indices = [i for i, d in enumerate(example.desirable) if d]
            if desirable_indices:
                example.sft_index = desirable_indices[0]
            else:
                example.sft_index = max(range(len(example.scores)), key=lambda i: example.scores[i])

            data[prompt_key] = example
            
        elif feedback_type == 'pairwise_feedback':
            example = Example()
            
            # Track unique outputs to avoid duplicates
            output_to_idx = {}
            
            for pair in samples:
                example.prompt = pair['prompt']
                example.dataset_name = 'feedback'

                # Add outputs if not already added
                if pair['output_A'][0]['content'] not in output_to_idx:
                    output_to_idx[pair['output_A'][0]['content']] = len(example.generations)
                    example.generations.append(pair['output_A'])
                    example.scores.append(pair['reward_A'])
                    
                if pair['output_B'][0]['content'] not in output_to_idx:
                    output_to_idx[pair['output_B'][0]['content']] = len(example.generations)
                    example.generations.append(pair['output_B'])
                    example.scores.append(pair['reward_B'])
                
                # Add preference pair as a tuple of indices
                if pair['label'] == 1:
                    example.pairs.append((output_to_idx[pair['output_A'][0]['content']], output_to_idx[pair['output_B'][0]['content']]))
                else:
                    example.pairs.append((output_to_idx[pair['output_B'][0]['content']], output_to_idx[pair['output_A'][0]['content']]))
            
            # Use highest scoring response as SFT target
            example.sft_index = max(range(len(example.scores)), key=lambda i: example.scores[i])
            
            data[prompt_key] = example
        else:
            raise ValueError(f"Unsupported feedback type: {feedback_type}")
    
    return data

def get_hendrycks_MATH(split: str = "test") -> Dataset:
    """
    Load the "nlile/hendrycks-MATH-benchmark" dataset and convert it into a Dataset.
    
    Args:
        split: one of 'test', 'train' (default: 'test')
        
    Returns:
        A Dataset instance containing MATH-500 problems.
    """
    
    rank0_print(f'Loading hendrycks_MATH dataset ({split} split) from Huggingface...')
    
    # Load the dataset
    dataset = datasets.load_dataset("nlile/hendrycks-MATH-benchmark", split=split)
    data = Dataset('hendrycks_MATH')
    
    if on_rank0():
        dataset = tqdm.tqdm(dataset, desc='Processing hendrycks_MATH')
    
    for row in dataset:
        problem = row['problem']
        solution = row['solution']
        answer = row['answer']
        
        # Create a conversation format with the problem as the human prompt
        conversation = [
            {"role": "user", "content": problem}
        ]
        
        # Use the problem as the key
        data[problem].prompt = conversation
        
        # Add the solution and answer as the assistant's response
        response = f"Answer:\n{solution}"
        data[problem].generations.append([{"role": "assistant", "content": response}])
        data[problem].answer = answer
        # Set this as the preferred response (for SFT)
        data[problem].sft_index = 0    
    return data

class DataLoader:
    """
    The base data loader class, similar to the one from the DPO repo.
    Subclass this and overwrite the __iter__ method as needed, since the batcch elements will be different depending
    on whether you're doing SFT, aligning with a pairwise loss like DPO, or alignment with an unpaired loss like KTO. 
    """
    def __init__(self, 
                 dataset_names: List[str],
                 tokenizer,
                 process_index: int = 0,
                 num_processes: int = 1,
                 split: str = 'train',
                 microbatch_size: int = 1,
                 max_length: int = 512,
                 max_prompt_length: int = 128,
                 max_prompt_count: int = None,
                 n_epochs: Optional[int] = None,
                 n_examples: Optional[int] = None,
                 seed: int = 0,
                 control_tokens: Dict = {},
                 use_chat_template: bool = False,
                 **kwargs):
        
        torch.manual_seed(seed)
        self.rng = random.Random(seed)
        self.tokenizer = tokenizer
        self.process_index = process_index
        self.num_processes = num_processes
        self.control_tokens = control_tokens
        self.split = split
        self.microbatch_size = microbatch_size
        self.max_length = max_length
        self.max_prompt_length = max_prompt_length
        self.max_prompt_count = max_prompt_count
        self.use_chat_template = use_chat_template
        self.kwargs = kwargs

        assert n_epochs is not None or n_examples is not None, "Must specify either n_epochs or n_examples"
        self.n_epochs = n_epochs
        self.epoch_idx = 0
        self.n_examples = n_examples
        
        self.full_data = {} # a dict of Examples

        for name in dataset_names:
            if f"get_{name}" in globals():
                dataset = globals()[f"get_{name}"](split)
                self.full_data.update(dataset.data)
            else:
                try:
                    with open(name, 'r') as f:
                        data = json.load(f)

                        if data[0]['type'] == 'sample':
                            dataset = get_sampled_data(name, split)
                        elif data[0]['type'].endswith('feedback'):
                            dataset = get_feedback(name, split)
                        else:
                            raise IOError("unrecognized data type")
                        
                        self.full_data.update(dataset.data)
                except:
                    raise IOError(f"could not load {name}")

        self.num_training_steps = self.get_num_training_steps()

    def collate(self, batch: Dict[str, List]) -> Dict:
        """
        Takes a list of examples and returns a batch of examples with consistent padding across all processes.
        Uses a fixed maximum length for padding to ensure consistency across batches and processes.
        """
        if self.tokenizer.pad_token_id is None:
            raise Exception("tokenizer's pad_token_id is not specified")
        
        padded_batch = {}
        for k in batch[0].keys():
            if k.endswith('_input_ids') or k.endswith('_attention_mask') or k.endswith('_labels'):
                if 'prompt' in k:
                    # flip prompt so that you are padding to the beginning
                    to_pad = [torch.LongTensor(ex[k][::-1]) for ex in batch]
                else:
                    to_pad = [torch.LongTensor(ex[k]) for ex in batch]

                if k.endswith('_input_ids'):
                    padding_value = self.tokenizer.pad_token_id
                elif k.endswith('_labels'):
                    padding_value = -100
                elif k.endswith('_attention_mask'):
                    padding_value = 0
                else:
                    raise ValueError(f"Unexpected key in batch '{k}'")

                # Always pad to max_length for consistency across processes
                max_len = self.max_prompt_length if 'prompt' in k else self.max_length

                padded_sequences = []
                for seq in to_pad:
                    if len(seq) > max_len:
                        padded_seq = seq[:max_len]
                    else:
                        padding_size = max_len - len(seq)
                        padding = torch.full((padding_size,), padding_value, dtype=seq.dtype)
                        padded_seq = torch.cat([seq, padding])
                    padded_sequences.append(padded_seq)

                padded_batch[k] = torch.stack(padded_sequences)
                if 'prompt' in k:
                    padded_batch[k] = padded_batch[k].flip(dims=[1])
            elif k == "answer":
                padded_batch[k] = [ex[k] for ex in batch]
            else:
                padded_batch[k] = [ex[k] for ex in batch]

        return padded_batch

    def tokenize_batch_element(self, conversation: List[Dict[str, str]], generation: List[Dict[str, str]], truncation_mode: str, prefix: str='target') -> Dict:
        """
        Tokenize a single batch element and truncate if prompt + generation is too long. Batch element is turned into Pytorch
        tensors in self.collate. Create the labels for the generation, which are of length equal to the sum of the length of
        the prompt and the generation, with -100 for the prompt tokens.

        Handles both chat-template-based tokenization and raw text tokenization based on self.use_chat_template.

        Args:
        - conversation: list of previous turns, each resembling dict {"role": "user/assistant", "content": text}
        - generation: list of current turns, typically one dict: [{"role": "assistant", "content": text}]
        - truncation_mode: one of 'keep_start'/'keep_end' (truncate end/beginning of prompt respectively if prompt is too long)
        - prefix: the prefix corresponding to the generation (e.g., 'chosen', 'rejected', 'target')

        Returns:
            A dict containing tokenized inputs, attention masks, labels, and original text components.
            Keys related to the specific generation have the specified `prefix`.
            Padding is handled later in the `collate` function.
        """
        batch_element = {} # Initialize the dictionary for the return value

        # Ensure conversation and generation are not None or malformed, provide defaults
        if not conversation: conversation = [{"role": "user", "content": ""}]
        if not generation: generation = [{"role": "assistant", "content": ""}]


        if self.use_chat_template:
            # --- Path for Chat Models using Templates ---

            # Note: The original truncation logic modifies the input lists in place.
            # This is kept here to match the provided code, but consider refactoring
            # to avoid side effects in the future.
            conversation_copy = [turn.copy() for turn in conversation] # Work on copies if possible
            generation_copy = [turn.copy() for turn in generation] # Work on copies if possible

            # Store original untruncated prompt string (as generated by template) for potential use (e.g., logging, inference setup)
            untruncated_prompt_string = self.tokenizer.apply_chat_template(conversation_copy, tokenize=False, add_generation_prompt=True)

            filter_out_bos_eos_pad = lambda x: [ t for t in x if t not in [ self.tokenizer.bos_token_id, self.tokenizer.eos_token_id, self.tokenizer.pad_token_id] ]

            current_total_length = 0
            truncated_conversation = []
            # Truncate conversation history turns if needed to fit max_prompt_length
            for i, turn in enumerate(conversation_copy):
                # Estimate length added by this turn's template + content
                # Use apply_chat_template on the single turn for accurate length estimation including template tokens
                templated_turn_ids = self.tokenizer.apply_chat_template([turn], tokenize=True, add_generation_prompt=(i == len(conversation_copy) - 1)) # Add prompt only for last user turn normally
                templated_length = len(templated_turn_ids)

                # Check if adding this turn exceeds max_prompt_length
                if current_total_length + templated_length > self.max_prompt_length:
                    # Calculate remaining space
                    remaining_space = self.max_prompt_length - current_total_length
                    # Re-tokenize just the content to find where to cut
                    content_token_ids = filter_out_bos_eos_pad(self.tokenizer.encode(turn['content'], add_special_tokens=False)) # Tokenize content only

                    # Estimate how many content tokens can fit. This is approximate.
                    # A more robust way involves carefully subtracting template token counts.
                    # For simplicity, let's assume template tokens take some overhead T: len = T + len(content_tokens)
                    # We need content_tokens <= remaining_space - T. T is hard to know exactly.
                    # Let's use a simpler heuristic: truncate content tokens based on remaining_space.
                    # This might slightly over/under shoot.
                    # A simpler approach: just truncate the templated_turn_ids
                    if remaining_space > 0:
                        truncated_turn_ids = templated_turn_ids[:remaining_space]
                        # Try to decode back - this might fail if cut mid-template
                        try:
                           turn['content'] = self.tokenizer.decode(filter_out_bos_eos_pad(truncated_turn_ids), skip_special_tokens=True) # Attempt to recover content
                        except:
                           # If decoding fails, maybe just stop adding turns? Or use a simpler content cut.
                           # Fallback: Rough content cut based on available space vs original content tokens
                           approx_overhead = templated_length - len(content_token_ids)
                           content_limit = max(0, remaining_space - approx_overhead)
                           turn['content'] = self.tokenizer.decode(content_token_ids[:content_limit])

                        truncated_conversation.append(turn)
                        current_total_length = self.max_prompt_length # Mark as full
                    break # Stop adding turns
                else:
                    truncated_conversation.append(turn)
                    current_total_length += templated_length # Update length based on templated version

            conversation_to_use = truncated_conversation
            # Use the calculated length after conversation truncation as the base for generation truncation
            current_total_length = len(self.tokenizer.apply_chat_template(conversation_to_use, tokenize=True, add_generation_prompt=True))


            truncated_generation = []
            # Truncate generation turns if needed to fit max_length (prompt + generation)
            for i, turn in enumerate(generation_copy):
                # Estimate length added by this turn's template + content
                # No add_generation_prompt needed when templating assistant turns usually
                templated_turn_ids = self.tokenizer.apply_chat_template([turn], tokenize=True, add_generation_prompt=False)
                templated_length = len(templated_turn_ids)

                if current_total_length + templated_length > self.max_length:
                     # Calculate remaining space
                    remaining_space = self.max_length - current_total_length
                    if remaining_space > 0:
                       # Similar truncation logic as above, applied to generation turn
                        truncated_turn_ids = templated_turn_ids[:remaining_space]
                        try:
                           turn['content'] = self.tokenizer.decode(filter_out_bos_eos_pad(truncated_turn_ids), skip_special_tokens=True)
                        except:
                            content_token_ids = filter_out_bos_eos_pad(self.tokenizer.encode(turn['content'], add_special_tokens=False))
                            approx_overhead = templated_length - len(content_token_ids)
                            content_limit = max(0, remaining_space - approx_overhead)
                            turn['content'] = self.tokenizer.decode(content_token_ids[:content_limit])

                        truncated_generation.append(turn)
                        current_total_length = self.max_length # Mark as full
                    break # Stop adding turns
                else:
                    truncated_generation.append(turn)
                    current_total_length += templated_length

            generation_to_use = truncated_generation

            # Final tokenization using the (potentially truncated) conversation and generation
            tokenized_prompt = self.tokenizer.apply_chat_template(conversation_to_use, tokenize=True, add_generation_prompt=True)
            tokenized_prompt_and_generation = self.tokenizer.apply_chat_template(
                conversation_to_use + generation_to_use,
                tokenize=True,
                add_generation_prompt=False # Template typically handles this transition
            )
            # Get string representations (for logging or inspection)
            final_prompt_string = self.tokenizer.apply_chat_template(conversation_to_use, tokenize=False, add_generation_prompt=True)
            final_generation_string = self.tokenizer.apply_chat_template(generation_to_use, tokenize=False, add_generation_prompt=False)
            final_combined_string = self.tokenizer.apply_chat_template(conversation_to_use + generation_to_use, tokenize=False, add_generation_prompt=False)

            # Prepare labels
            labels = list(tokenized_prompt_and_generation) # Create a mutable copy
            prompt_length = len(tokenized_prompt)
            # Mask out the prompt tokens
            labels[:prompt_length] = [-100] * prompt_length
            # Ensure labels are not longer than the combined sequence (shouldn't happen with correct tokenization)
            labels = labels[:len(tokenized_prompt_and_generation)]

            batch_element = {
                'prompt': conversation, # Original conversation structure
                f'{prefix}': generation, # Original generation structure
                'prompt_text': final_prompt_string, # Potentially truncated prompt text
                'prompt_input_ids': tokenized_prompt,
                f'{prefix}_text': final_generation_string, # Potentially truncated generation text
                f'{prefix}_combined_text': final_combined_string, # Potentially truncated combined text
                f'{prefix}_combined_input_ids': tokenized_prompt_and_generation,
                f'{prefix}_combined_attention_mask': [1] * len(tokenized_prompt_and_generation),
                f'{prefix}_labels': labels,
                # Include untruncated prompt string for reference if needed elsewhere
                # 'untruncated_prompt_text': untruncated_prompt_string
            }

        else:
            # --- Path for Base Models (No Template) ---

            # Extract raw text - assuming simple structure for base model fine-tuning
            prompt_str = conversation[0]['content'] if conversation else ""
            gen_str = generation[0]['content'] if generation else ""

            # Tokenize prompt (typically add BOS)
            # Check tokenizer documentation for default behavior (add_special_tokens=True usually adds BOS for Llama)
            tokenized_prompt = self.tokenizer.encode(prompt_str, add_special_tokens=True)

            # Apply prompt truncation if needed
            if len(tokenized_prompt) > self.max_prompt_length:
                if truncation_mode == 'keep_end':
                    # Keep the end, including the initial BOS if present
                    keep_tokens = tokenized_prompt[0:1] + tokenized_prompt[-(self.max_prompt_length-1):] if self.tokenizer.bos_token_id == tokenized_prompt[0] and self.max_prompt_length > 0 else tokenized_prompt[-self.max_prompt_length:]
                    tokenized_prompt = keep_tokens
                else: # 'keep_start' or default
                    tokenized_prompt = tokenized_prompt[:self.max_prompt_length]
                # Update prompt_str if needed for consistency (optional)
                # prompt_str = self.tokenizer.decode(tokenized_prompt, skip_special_tokens=True) # Decode without special tokens for clean text

            # Tokenize generation separately (without special tokens unless it's meant to be a standalone sequence)
            tokenized_generation = self.tokenizer.encode(gen_str, add_special_tokens=False) # No BOS/EOS for the middle part

            # Combine prompt and generation tokens
            tokenized_combined = tokenized_prompt + tokenized_generation

            # Apply combined truncation (prioritize keeping prompt)
            if len(tokenized_combined) > self.max_length:
                prompt_len = len(tokenized_prompt)
                max_gen_len = self.max_length - prompt_len
                if max_gen_len <= 0:
                    # Prompt alone is too long, truncate combined to max_length (effectively truncating prompt further)
                    tokenized_combined = tokenized_prompt[:self.max_length]
                else:
                    # Truncate the generation part
                    truncated_gen_tokens = tokenized_generation[:max_gen_len]
                    tokenized_combined = tokenized_prompt + truncated_gen_tokens
                # Update gen_str and combined_str if needed for consistency (optional)
                # gen_str = self.tokenizer.decode(tokenized_combined[prompt_len:], skip_special_tokens=True)

            # Add EOS token if the tokenizer doesn't add it automatically and it's expected by the model
            # Many Causal LMs expect EOS at the end of the full sequence.
            if self.tokenizer.eos_token_id is not None and (not tokenized_combined or tokenized_combined[-1] != self.tokenizer.eos_token_id):
                 # Check length before adding EOS
                 if len(tokenized_combined) < self.max_length:
                     tokenized_combined.append(self.tokenizer.eos_token_id)
                 elif len(tokenized_combined) == self.max_length:
                      # Replace the last token with EOS if already at max length
                      tokenized_combined[-1] = self.tokenizer.eos_token_id
                 # Else: EOS cannot be added as it's already over length after truncation

            # Create labels
            labels = list(tokenized_combined) # Create a mutable copy
            prompt_length = len(tokenized_prompt)
            # Mask out the prompt part
            labels[:prompt_length] = [-100] * prompt_length
             # Ensure labels match the final tokenized length
            labels = labels[:len(tokenized_combined)]

            # Get final string representations (optional, for inspection)
            final_prompt_str = self.tokenizer.decode(tokenized_prompt, skip_special_tokens=True) # Clean prompt text
            final_gen_str = self.tokenizer.decode(tokenized_combined[prompt_length:], skip_special_tokens=True) # Clean gen text
            final_combined_str = self.tokenizer.decode(tokenized_combined, skip_special_tokens=True) # Clean combined text

            batch_element = {
                'prompt': conversation, # Original structured prompt
                f'{prefix}': generation, # Original structured generation
                'prompt_text': final_prompt_str,
                'prompt_input_ids': tokenized_prompt,
                f'{prefix}_text': final_gen_str,
                f'{prefix}_combined_text': final_combined_str,
                f'{prefix}_combined_input_ids': tokenized_combined,
                f'{prefix}_combined_attention_mask': [1] * len(tokenized_combined),
                f'{prefix}_labels': labels,
            }

        # --- Common Post-processing (if any) ---
        # Example: Ensure no unexpected tokens remain (handled by filter_out_bos_eos_pad earlier for chat)
        # The padding itself is done in the collate function.

        return batch_element
    
    def __iter__(self):
        """Create a flat version of the data and yield batches."""
        raise NotImplementedError

    def get_num_training_steps(self):
        """Get the number of training steps."""
        raise NotImplementedError
    

class SFTDataLoader(DataLoader):
    """
    Dataloader for supervised fine-tuning.
    """
    def __iter__(self):
        flat_data = []
        prompts = list(self.full_data.keys())
        
        for prompt in prompts:
            flat_data.append(self.full_data[prompt])

        if self.num_processes == 1: # for eval usually
            usable_size = len(flat_data)
        else: # to avoid hanging with uneven batches
            global_batch_size = int(self.num_processes * self.microbatch_size)
            usable_size = len(flat_data) // global_batch_size * global_batch_size
        
        self.rng.shuffle(flat_data)
        flat_data = [d for i, d in enumerate(flat_data[:usable_size]) if i % self.num_processes == self.process_index]

        epoch_idx = 0
        example_idx = 0
        done = False
        
        while True:
            if done: break
            self.rng.shuffle(flat_data)

            batch = []

            for example in flat_data:
                # Assuming example.prompt is now a list of conversation turns
                conversation = example.prompt
                if not isinstance(conversation[0], dict):
                    # Convert to the new format if it's not already
                    conversation = [{"role": "user", "content": conversation[0]}]
                    for i, message in enumerate(conversation[1:]):
                        role = "assistant" if i % 2 == 0 else "user"
                        conversation.append({"role": role, "content": message})

                # Get the target generation (last turn from assistant)
                target_generation = example.generations[example.sft_index]

                # Add control token if specified
                if self.control_tokens.get('chosen'):
                    target_generation = self.control_tokens['chosen'] + target_generation

                batch_element = self.tokenize_batch_element(
                    conversation,
                    target_generation,
                    example.truncation_mode
                )
                batch_element['original_prompt'] = example.original_prompt
                #for gt_answer tracking
                batch_element['answer'] = example.answer
                batch.append(batch_element)

                if len(batch) == self.microbatch_size:
                    example_idx += len(batch) * self.num_processes
                    yield self.collate(batch)
                    batch = []

                    if self.n_examples is not None and example_idx >= self.n_examples:
                        rank0_print(f'Finished generating {self.n_examples} examples on {self.split} split')
                        done = True
                        break

            if self.num_processes == 1 and batch != []: # flush for eval, sampling
                yield self.collate(batch) 
                batch = []

            epoch_idx += 1
            if self.n_epochs is not None and epoch_idx >= self.n_epochs:
                done = True
                break

    def get_num_training_steps(self):
        """Get the number of training steps."""
        return len(self.full_data) // self.num_processes


class ConditionalSFTDataLoader(DataLoader):
    """
    Dataloader for token-conditioned SFT, in the style of Korbak et al.'s (2023) "Pretraining Models with Human
    Feedback."

    For training, each output is prepended with a control token denoting whether it's desirable or undesirable
    (<|good|> or <|bad|> respectively). For sampling, each input is postpended with the <good> token to ensure
    that only desirable outputs are generated.
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        if self.control_tokens.get('chosen') is None:
            raise KeyError("control token for chosen outputs not specified")
        
        if self.control_tokens.get('rejected') is None:
            raise KeyError("control token for rejected outputs not specified")

    def get_flat_data(self, prompts):
        """
        Return a flat list of examples given a list of prompts that index self.full_data.
        Prepend the examples with the appropriate control tokens.
        """
        flat_data = []

        for prompt in prompts:
            example = self.full_data[prompt]

            if self.max_prompt_count:
                example.pairs = self.rng.sample(example.pairs, min(self.max_prompt_count, len(example.pairs)))

            for i,j in example.pairs:
                flat_data.append((example, example.generations[i], 'chosen'))
                flat_data.append((example, example.generations[j], 'rejected'))

        return flat_data
    
    def __iter__(self):
        prompts = list(self.full_data.keys())
        flat_data = self.get_flat_data(prompts)

        if self.num_processes == 1: # for eval usually
            usable_size = len(flat_data)
        else: # to avoid hanging with uneven batches
            global_batch_size = int(self.num_processes * self.microbatch_size)
            usable_size = len(flat_data) // global_batch_size * global_batch_size
        
        self.rng.shuffle(flat_data) # shuffle before splitting across processes, otherwise some processes will only get chosen examples
        flat_data = [d for i, d in enumerate(flat_data[:usable_size]) if i % self.num_processes == self.process_index]
      
        epoch_idx = 0
        example_idx = 0
        done = False
        
        while True:
            if done: break
            self.rng.shuffle(flat_data)

            batch = []

            for example, generation, status in flat_data:
                # Convert prompt to conversation format if it's not already
                conversation = example.prompt
                if not isinstance(conversation[0], dict):
                    conversation = [{"role": "user", "content": conversation[0]}]
                    for i, message in enumerate(conversation[1:]):
                        role = "assistant" if i % 2 == 0 else "user"
                        conversation.append({"role": role, "content": message})

                # Add control token to the generation
                if status == 'chosen':
                    conditioned_generation = self.control_tokens["chosen"] + generation
                else:
                    conditioned_generation = self.control_tokens["rejected"] + generation

                batch_element = self.tokenize_batch_element(
                    conversation,
                    conditioned_generation,
                    example.truncation_mode
                )
                batch_element['status'] = status
                batch.append(batch_element)

                if len(batch) >= self.microbatch_size:
                    example_idx += len(batch) * self.num_processes
                    yield self.collate(batch)
                    batch = []

                    if self.n_examples is not None and example_idx >= self.n_examples:
                        rank0_print(f'Finished generating {example_idx} examples on {self.split} split')
                        done = True
                        break

            if self.num_processes == 1 and batch != []: # flush for eval, sampling
                yield self.collate(batch)
                batch = []

            epoch_idx += 1
            if self.n_epochs is not None and epoch_idx >= self.n_epochs:
                done = True
                break

    def get_num_training_steps(self):
        max_prompt_count = min(float("inf"), self.max_prompt_count) if self.max_prompt_count else float("inf")
        num_pairs = int(sum(min(max_prompt_count, len(example.pairs)) for _, example in self.full_data.items()))
        num_training_steps = num_pairs * 2
        return num_training_steps // self.num_processes


class UnpairedPreferenceDataLoader(DataLoader):
    """
    Dataloader for losses that do not require pairwise preferences (e.g., KTO).

    This assumes that if an example has no pairs, then it is naturally unpaired, using the 'desirable' field
    to infer its label. If an example has pairs, then it is assumed to be from a naturally paired dataset, and 
    the preferred/dispreferred generations are from the desirable/undesirable conditional generations given x. 
    """
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)

        if self.microbatch_size * self.num_processes <= 1:
            raise ValueError("can't use batch size of 1 with UnpairedPreferenceDataLoader")
        
    def get_flat_data(self, prompts):
        """
        Return a flat list of examples given a list of prompts that index self.full_data.
        """
        if self.max_prompt_count:
            num_unique = sum(min(self.max_prompt_count, len(self.full_data[prompt].generations)) for prompt in prompts)
        else:
            num_unique = sum(len(self.full_data[prompt].generations) for prompt in prompts)

        allowed_desirable = num_unique * self.kwargs.get('frac_unique_desirable', np.inf)
        allowed_undesirable = num_unique * self.kwargs.get('frac_unique_undesirable', np.inf)
        seen_desirable = 0
        seen_undesirable = 0

        flat_data = []

        for prompt in prompts:
            example = self.full_data[prompt]

            # naturally unpaired feedback
            if example.pairs == [] and example.desirable != []:
                for i in range(len(example.desirable)):
                    if seen_desirable < allowed_desirable and example.desirable[i]:
                        flat_data.append((example, example.generations[i], 'chosen'))
                        seen_desirable += 1

                    if seen_undesirable < allowed_undesirable and not example.desirable[i]:
                        flat_data.append((example, example.generations[i], 'rejected'))
                        seen_undesirable += 1
            # getting unpaired data out of pairs
            elif example.pairs != []:
                if self.max_prompt_count:
                    example.pairs = self.rng.sample(example.pairs, min(self.max_prompt_count, len(example.pairs)))

                for i,j in example.pairs:
                    if seen_desirable < allowed_desirable:
                        flat_data.append((example, example.generations[i], 'chosen'))
                        seen_desirable += 1
                    
                    if seen_undesirable < allowed_undesirable:
                        flat_data.append((example, example.generations[j], 'rejected'))
                        seen_undesirable += 1
            else:
                raise IOError("data is neither paired nor has desirability labels")

        return flat_data

    def __iter__(self):
        prompts = list(self.full_data.keys())
        flat_data = self.get_flat_data(prompts)

        if self.num_processes == 1: # for eval usually
            usable_size = len(flat_data)
        else: # to avoid hanging with uneven batches
            global_batch_size = int(self.num_processes * self.microbatch_size)
            usable_size = len(flat_data) // global_batch_size * global_batch_size
        
        self.rng.shuffle(flat_data) # shuffle before splitting across processes, otherwise some processes will only get chosen examples
        flat_data = [d for i, d in enumerate(flat_data[:usable_size]) if i % self.num_processes == self.process_index]

        epoch_idx = 0
        example_idx = 0
        done = False

        while True:
            if done: break
            self.rng.shuffle(flat_data)   # so generations in the same preference are not in the same batch
            batch = []
            example_queue = []

            for example, generation, status in flat_data:
                batch_element = self.tokenize_batch_element(example.prompt, generation, example.truncation_mode, prefix='target')
                batch_element['status'] = status 
                batch_element['truncation_mode'] = example.truncation_mode
                batch_element['conversation'] = example.prompt
                batch_element['generation'] = generation
                batch_element['advantage'] = example.advantage 
                example_queue.append(batch_element)
                
                if len(example_queue) >= self.microbatch_size:
                    while len(batch) < self.microbatch_size:
                        batch.append(example_queue.pop(0))
                    
                if len(batch) >= self.microbatch_size:
                    # for estimating the KL term, match up x and y' that are not corresponding input-output pairs in the data
                    # for x_i, get a mismatched y' by just picking the subsequent y_{i+1} in the batch (desirable/undesirable status does not matter)
                    # the respective input IDs, attention mask, and so on will be prefixed by the term KL
                    indices = list(range(1, len(batch))) + [0]
                    for i in range(len(batch)):
                        batch[i].update(self.tokenize_batch_element(
                            batch[i]['conversation'],
                            batch[indices[i]]['generation'],
                            batch[i]['truncation_mode'],
                            prefix='KL'
                        ))

                    example_idx += len(batch) * self.num_processes
                    yield self.collate(batch)
                    batch = []

                    if self.n_examples is not None and example_idx >= self.n_examples:
                        rank0_print(f'Finished generating {example_idx} examples on {self.split} split')
                        done = True
                        break

            if self.num_processes == 1 and batch != []: # flush for eval, sampling
                yield self.collate(batch)
                batch = []

            epoch_idx += 1
            if self.n_epochs is not None and epoch_idx >= self.n_epochs:
                done = True
                break

    def get_num_training_steps(self):
        max_prompt_count = min(float("inf"), self.max_prompt_count) if self.max_prompt_count else float("inf")
        num_pairs = int(sum(min(max_prompt_count, len(example.pairs)) for _, example in self.full_data.items()))
        num_training_steps = num_pairs * self.kwargs.get('frac_unique_desirable', 1.0) + num_pairs * self.kwargs.get('frac_unique_undesirable', 1.0)
        return num_training_steps // self.num_processes


class ScoreDataLoader(UnpairedPreferenceDataLoader):
    def get_flat_data(self, prompts):
        """
        Return a flat list of examples given a list of prompts that index self.full_data.
        Assumes that there are a list of scores.
        """
        flat_data = []
        prev_status = 'rejected'

        for prompt in prompts:
            example = self.full_data[prompt]

            if self.max_prompt_count:
                example.pairs = self.rng.sample(example.pairs, min(self.max_prompt_count, len(example.pairs)))

            # for oasst, lower scores are better, so rank 0 is the best response and rank n is the worst
            if prev_status == 'rejected':
                flat_data.append((example, example.generations[np.argmin(example.scores)], 'chosen'))
            else:
                flat_data.append((example, example.generations[np.argmax(example.scores)], 'rejected'))

            prev_status = flat_data[-1][-1]

        return flat_data


class HalfPrefDataLoader(UnpairedPreferenceDataLoader):
    """
    Dataloader for training on only one output per input.
    This throws out at least half the data (more than half if there are multiple pairs per input).
    For this reason, this should ONLY be used for training.
    """
    def get_flat_data(self, prompts):
        """
        Return a flat list of examples given a list of prompts that index self.full_data.
        Only use one preference pair per input.
        """
        flat_data = []
        prev_status = 'rejected'

        for prompt in prompts:
            example = self.full_data[prompt]

            if self.max_prompt_count:
                example.pairs = self.rng.sample(example.pairs, min(self.max_prompt_count, len(example.pairs)))

            for i,j in example.pairs:
                if prev_status == 'rejected':
                    flat_data.append((example, example.generations[i], 'chosen'))
                else:
                    flat_data.append((example, example.generations[j], 'rejected'))

                prev_status = flat_data[-1][-1]
                break # only use one pair

        return flat_data


class PairedPreferenceDataLoader(DataLoader):
    """
    Dataloader for losses that do require pairwise preferences (e.g., DPO).
    """
    def __iter__(self):
        prompts = list(self.full_data.keys())
        flat_data = []

        for prompt in prompts:
            example = self.full_data[prompt]

            if self.max_prompt_count:
                example.pairs = self.rng.sample(example.pairs, min(self.max_prompt_count, len(example.pairs)))

            for pair in example.pairs:
                flat_data.append((example, pair))

        if self.num_processes == 1: # for eval, sampling
            usable_size = len(flat_data)
        else: # to avoid hanging with uneven batches
            global_batch_size = int(self.num_processes * self.microbatch_size)
            usable_size = len(flat_data) // global_batch_size * global_batch_size

        self.rng.shuffle(flat_data) # shuffle before splitting across processes, otherwise some processes will only get chosen examples
        flat_data = [d for i, d in enumerate(flat_data[:usable_size]) if i % self.num_processes == self.process_index]
         
        epoch_idx = 0
        example_idx = 0
        done = False

        while True:
            if done: break
            self.rng.shuffle(flat_data)
            batch = []

            for example, (i, j) in flat_data:
                batch_element = {}
                batch_element.update(self.tokenize_batch_element(example.prompt, example.generations[i], example.truncation_mode, prefix='chosen'))
                batch_element.update(self.tokenize_batch_element(example.prompt, example.generations[j], example.truncation_mode, prefix='rejected'))
                batch.append(batch_element)

                if len(batch) >= self.microbatch_size:
                    example_idx += len(batch) * self.num_processes
                    yield self.collate(batch)
                    batch = []

                    if self.n_examples is not None and example_idx >= self.n_examples:
                        rank0_print(f'Finished {example_idx} examples on {self.split} split')
                        done = True
                        break

            if self.num_processes == 1 and batch != []: # flush for eval, sampling
                yield self.collate(batch)
                batch = []

            epoch_idx += 1
            if self.n_epochs is not None and epoch_idx >= self.n_epochs:
                done = True
                break

    def get_num_training_steps(self):
        max_prompt_count = min(float("inf"), self.max_prompt_count) if self.max_prompt_count else float("inf")
        all_data = int(sum(min(max_prompt_count, len(example.pairs)) for _, example in self.full_data.items()))
        return all_data // self.num_processes
